Learning approach

1. supervised learning
  Regression
    -> predict a continuous output
  Classification
    -> predict a discrete output

features/attributes

2. un-supervised
  Clustering
  Non-clustering
    -> cocktail party problem


Framework
  GNU Octave
    -> learning tool / prototype tool

========================================

Model Representation

1. hypothesis
  h_theta(x)
    Training Set
	       |
	Learning Algorithm
		   |
	hypothesis (maps from input to output)
	
	Input --> hypothesis --> prediction

2. cost function
  J(theta)	

========================================

Gradient descent:
  Theta_j := Theta_j - alpha * J(theta) derivative theta
  req: simultaneous update to all feature.
  * alpha -> learning rate
  * derivative -> J(theta) d_theta

  converge with fixed learning rate (alpha)
    since derivative actually reduce through iteration.

  drawbacks:
    converges to local optimization instead of global optimization

  "Batch" Gradient Descent:
    each step uses all training examples

  compare to solving for optimal:
    Gradient descent actually scales better
