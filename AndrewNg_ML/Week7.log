==Support Vector Machine================================================

--Alternative view of logistic regression-------------------------------
* h_theta(x) = 1 / (1+exp(-Theta.T * x))
  if y = 1, we want h_theta(x) closer to 1, Theta.T*x >> 0
  similarly, if y = 0, Theta.T*x << 0

* cost of example (cost function?):
  -y*log(1/(1+exp(-Theta.T*x))) - (1-y)*log(1-1/(1+exp(-Theta.T*x)))
             cost of 1                         cost of 0
  replace cost function with:
    cost_1(z) and cost_0(z)

* Support Vector Machine
  Cost Function:
  min_Theta C*sum_i(y[i]cost_1(Theta.T*x[i])+(1-y[i])cost_0(Theta.T*x))
			+ 1/2*sum_j(Theta_j^2)

  Hypothesis:
  h_Theta(x) = 1 ,  if Theta.T*x >= 0
			   0 ,  otherwise
  
--Large Margin Classifiers----------------------------------------------
* objective of SVM
    Theta.T*x >= 1 for y=1
    Theta.T*x <=-1 for y=0

* SVM Decision Boundary
  - suppose C is very large
    Theta.T*x >= 1 for y=1
    Theta.T*x <=-1 for y=0

    * optimization problem becomes
	  min_Theta 	1/2*sum_j(Theta_j^2)
	  S.T. 			Theta.T*x[i] >= 1 if y[i] = 1
	      			Theta.T*x[i] <=-1 if y[i] = 0
	optimization leads to 
	  Large margin classifier

  - adjust C to accommodate outliers

--The mathematics behind large margin classification--------------------
* vector inner product
  u.T * v = p * || u ||
    where as:
	  p = signed length of projection of v onto u
	
Formulation
  min_Theta    1/2*sum_j(Theta_j^2)
  S.T.         Theta.T*x[i] >= 1 if y[i] = 1
               Theta.T*x[i] <=-1 if y[i] = 0
			   
simplify SVM with Theta_0 = 0 and n = 2;
  Decision boundary is perpendicular to Theta;
  Margin is determined by projection of x onto Theta;
  
  Since we are minimizing ||Theta||
  -> maximize the minimum norm of projection of x onto Theta;
  -> we are maximizing the margin

--Kernel----------------------------------------------------------------
* Non-linear Decision Boundary
  - Other features than high order polynomial terms?

  for h_theta(x) = theta_0 + theta_1*x1 + theta_2*x2 + ...
  suppose instead of x1, x2, ...
    we have f1, f2, ... as our new feature

Prediction
* similarity function (kernel)
  Given x, compute new feature depending on proximity to landmarks l:
  f_i = similarity(x, l_i) = exp(-(||x-l_i||^2)/(2*sigma^2))
             kernel                  Gaussian Kernel
  
    * if x ~ l_i:
        f_i ~ 1
    * if x is far from l_i:
        f_i ~ 0

* prediction
  theta_0 + theta_1*f_1 + theta_2*f_2 + ... >= 0
    - predict 1
    - otherwise, predict 0

Training
* choosing landmarks
  Given (x[1], y[1]), (x[2], y[2]), ..., (x[m], y[m])
  choose l[1]=x[1], l[2]=x[2], ..., l[m]=x[m] 

  for x[i] -> compute f[i] = [f_0[i] f_1[i] f_2[i] ... f_m[i]].T
    where f_0[i] = 1
    f is R_m+1 (dimension m+1)

  Hypothesis
    Predict "y=1" if Theta.T * f >= 0

  Training:
    min_Theta
      C*sum_i(y[i]*cost1(Theta.T*f[i])+(1-y[i])*cost0(Theta.T*f[i])+
      1/2*sum_j(Theta_j^2)

  sum_j(Theta_j^2) = Theta.T*Theta
                and it is usually done as Theta.T*M*Theta
                this M is determined by kernel
                allows more efficient computation and better scalability

  * Parameters of SVM
    - C (=1/lambda) 
      Large C -> lower bias, high variance (small lambda)
      Small C -> higher bias, low variance (large lambda)
    - sigma^2
      Large sigma^2: features f_i smooth
        Higher bias, lower variance
      Small sigma^2: features f_i sharper
        Lower bias, higher variance

--Using SVM-------------------------------------------------------------
How
- Use SVM software package (e.g. liblinear, libsvm, ...) to solve Theta
- Choice of parameter C
- Choice of kernel (similarity function):
  e.g. No kernel ("linear kernel") where f_i = x_i
  e.g. Gaussian kernel
       * f_i = exp(-||x-l_i||^2 / (2*sigma^2)), where l_i = x_i
       * also need to choose sigma^2
       * Note: do perform feature scaling before using the Gaussian
         kernel
  other choices of kernel
    Note: not all similarity functions make valid kernels
          need to satisfy technical condition called "Mercer's Theorem"
          to make sure SVM packages' optimizations run correctly and do
          not diverge
    e.g. Polynomial kernel
    e.g. More esoteric: String kernel, chi-square kernel,
                        histogram intersection kernel

Multi-class classification
- Many SVM packages already have built-in multi-class classification
- Otherwise, use one-vs.-all method
  Train K SVMs, one to distinguish y=i from the rest

Logistic regression vs. SVMs
* suppose
    n = number of features
    m = number of training examples
  if n >> m:
    Use logistic regression, or SVM without a kernel
  if n is small, m is intermediate:
    Use SVM with Gaussian kernel
  if n is small, m is large:
    Create/add more features, then use logistic regression or SVM
    without a kernel
