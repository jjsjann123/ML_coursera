==Classification and Regression=========================================
--Classification--------------------------------------------------------
  y = 0 or 1
  regression will not work
  
  Logistic Regression: 0 <= h_theta(x) <= 1
  * this is a classification algorithm although it is called regression
  
--Logistic Regression---------------------------------------------------
i. Model
  h_theta(x) = g(transpose(theta)*x);
  whereas,
	g(z) = 1/(1+e^(-z)); # Sigmoid function / Logistic function

  interpretation:
  0 <= h_theta(x) <= 1
	h_theta(x) = estimated probability that y=1 on input x
	           = p(y=1|x;theta)

ii. Decision Boundary
  predict "y=1", if h_theta(x) >= 0.5
          "y=0", otherwise
  h_theta(x) >= 0 --> transpose(theta)*x >= 0;
  h_theta(x) < 0 --> transpose(theta)*x < 0;

  transpose(theta)*x >= 0
	this is a boundary on space R(x)
	this is not necesarrily a linear boundary

iii. Cost Function (for logistic regression)
  J(theta) = 1/m * sum_i( cost(h_theta(x(i)), y(i) )

  * the cost function for linear regression does not work here.
    we want a convex J(theta) -> so we could use Gradient Descend
  cost(h_theta(x), y) = -log(h_theta(x))      , if y=1
                        -log(1 - h_theta(x))  , if y=0
    * h_theta(x) is between [0,1]
    * for correct prediction h_theta(x) = y -> cost(h_theta(x), y) = 0;

iv. Simplified Cost function and Gradient Descent
  * remove the condition 
  cost(h_theta(x), y) = -ylog(h_theta(x)) - (1-y)log(1-h_theta(x))

  min_theta J(theta)
    Gradient Descent
    * algorithm looks identical to linear regression!
    * only difference is h_theta(x)
    * feature scaling still holds

--Advanced Optimization-------------------------------------------------
Given theta, we need to compute
  i. J(theta)
  ii. patial_derivative( J(theta) )
to find minimum J(theta)

Optimization algorithms: (aside from Gradient descent)
  * Conjugate gradient
  * BFGS
  * L-BFGS
  Advantage:
    1. No need to manually pick alpha
    2. Often faster than gradient descent
  Disadvantages -> more complex

  --> libraries usually provide such solver to do it for you.

--Multiclass Classification---------------------------------------------
i. One-vs-all (one-vs-rest):
  fit n classifier (one for each class)
    h[i]_theta(x) = P(y=i | x;theta), for i=1,2,...,n
  on a new input
    pick class i that
      max[i] h[i]_theta(x)

==Overfitting===========================================================
i. Underfit ("high bias")
ii. Overfit ("high variance")
  * too many features
  * the learned hypothesis fit training set very well but fail to
    generalize to new examples

--Addressing overfitting------------------------------------------------
i. Reduce number of features
  * manually select which features to keep
  * model selection algorithm
ii. Regularization
  * keep all the features, but reduce magnitude/values of parameters
    theta_j
  * works well when we have a lot of features, each of which contributes
    a bit to predicting y

--Cost function---------------------------------------------------------
i. Intuition:
  Penalize extra theta_j by adding them into cost function.

ii. Regularization
  Small values for parameters theta
    -> "simpler" hypothesis (smoother)
    -> less prone to overfitting

  modify cost function to penalize each theta
  J(theta)=1/2m*(sum_i((h_theta(x[i])-y[i])^2)+ lambda*sum_i(theta_i^2))
  where as,
    lambda - regularization parameter

--Regularized Linear Regression-----------------------------------------
i. Gradient descent
  theta_0 := theta_0 - alpha / m * sum_i((h_theta(x[i])-y[i])*x[i]_0)
  theta_j := theta_j - alpha / m * sum_i((h_theta(x[i])-y[i])*x[i]_j)
             + (1-alpha*lambda/m) * theta_j

ii. Normal equation
  theta = (transpose(X)*X + lambda*[0 and 1 on diagonal]).inverse *
           transpose(X)*y
  Non-invertibility
    using regularization would makesure the matrix in the parenthesis is
    for sure invertible.

--Regularized Logistic Regression---------------------------------------
i. Gradient descent
  theta_0 := theta_0 - alpha / m * sum_i((h_theta(x[i])-y[i])*x[i]_0)
  theta_j := theta_j - alpha / m * sum_i((h_theta(x[i])-y[i])*x[i]_j)
             + (1-alpha*lambda/m) * theta_j
  * notice this is not the same as Linear Regression. Because we have
    different h_theta(x)

ii. Advanced optimization
  * Only applies to Octave
  * We only need to update the cost function to theta.
